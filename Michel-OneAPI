#-------------------------1D AXPY

function axpy_oneapi_kernel(alpha::Float64, x, y)
  i = get_global_id()
  @inbounds x[i] = x[i] + alpha * y[i]
  return nothing
end

function axpy_oneapi(SIZE,alpha::Float64,x,y)
  maxPossibleItems = 256
  items = min(SIZE, maxPossibleItems)
  groups = ceil(Int, SIZE / items)
  oneAPI.@sync @oneapi items = items groups = groups axpy_oneapi_kernel(alpha,x,y)
end

SIZE = 100000000
x = ones(Float64,SIZE)
y = ones(Float64,SIZE)
alpha::Float64 = 2.0
dx = oneArray(x)
dy = oneArray(y)
for i in [10,100,1000,10000,100000,1000000,10000000,100000000]
 @time begin
  axpy_oneapi(i,alpha,dx,dy)
 end
end

function axpy(i, alpha, x, y)
  @inbounds x[i] += alpha * y[i]
end

x = ones(SIZE)
y = ones(SIZE)
jx = JACC.Array(x)
jy = JACC.Array(y)
for i in [10,100,1000,10000,100000,1000000,10000000,100000000]
 @time begin
  JACC.parallel_for(i, axpy, alpha, jx, jy)
 end
end

#-------------------------1D DOT

function dot_oneapi_kernel(SIZE, ret, x, y)
  shared_mem = oneLocalArray(Float64, 256)
  i = get_global_id(0)
  ti = get_local_id(0)
  tmp::Float64 = 0.0
  shared_mem[ti] = 0.0
  if i <= SIZE
    tmp = @inbounds x[i] * y[i]
    shared_mem[ti] = tmp
  end
  barrier() 
  if (ti <= 128)
    shared_mem[ti] += shared_mem[ti+128]
  end
  barrier() 
  if (ti <= 64)
    shared_mem[ti] += shared_mem[ti+64]
  end
  barrier()
  if (ti <= 32)
    shared_mem[ti] += shared_mem[ti+32]
  end
  barrier()
  if (ti <= 16)
    shared_mem[ti] += shared_mem[ti+16]
  end
  barrier()
  if (ti <= 8)
    shared_mem[ti] += shared_mem[ti+8]
  end
  barrier()
  if (ti <= 4)
    shared_mem[ti] += shared_mem[ti+4]
  end
  barrier()
  if (ti <= 2)
    shared_mem[ti] += shared_mem[ti+2]
  end
  barrier()
  if (ti == 1)
    shared_mem[ti] += shared_mem[ti+1]
    ret[get_group_id(0)] = shared_mem[ti]
  end
  barrier()
  return nothing
end

function reduce_kernel(SIZE, red, ret)
  shared_mem = oneLocalArray(Float64, 256)
  i = get_global_id(0)
  ii = i
  tmp::Float64 = 0.0
  if SIZE > 256
    while ii <= SIZE
      tmp += @inbounds red[ii]
      ii += 256
    end
  else
    tmp = @inbounds red[i]
  end
  shared_mem[i] = tmp
  barrier()
  if (i <= 128)
    shared_mem[i] += shared_mem[i+128]
  end
  barrier()
  if (i <= 64)
    shared_mem[i] += shared_mem[i+64]
  end
  barrier()
  if (i <= 32)
    shared_mem[i] += shared_mem[i+32]
  end
  barrier()
  if (i <= 16)
    shared_mem[i] += shared_mem[i+16]
  end
  barrier()
  if (i <= 8)
    shared_mem[i] += shared_mem[i+8]
  end
  barrier()
  if (i <= 4)
    shared_mem[i] += shared_mem[i+4]
  end
  barrier()
  if (i <= 2)
    shared_mem[i] += shared_mem[i+2]
  end
  barrier()
  if (i == 1)
    shared_mem[i] += shared_mem[i+1]
    ret[1] = shared_mem[1]
  end
  return nothing
end

function dot_oneapi(SIZE, x, y)
  numItems = 256
  items = min(SIZE, numItems)
  groups = ceil(Int, SIZE/items)
  ret = oneAPI.zeros(Float64, groups)
  rret = oneAPI.zeros(Float64, 1)
  oneAPI.@sync @oneapi items = items groups = groups dot_oneapi_kernel(SIZE, ret, x, y)
  oneAPI.@sync @oneapi items = items groups = 1 reduce_kernel(SIZE, ret, rret)
  return rret
end

SIZE = 100000000
x = ones(SIZE)
y = ones(SIZE)
dx = oneArray(x)
dy = oneArray(y)
for i in [10,100,1000,10000,100000,1000000,10000000,100000000]
 @time begin
  res = dot_oneapi(i, dx, dy)
 end
end

function dot(i, x, y)
  return @inbounds x[i] * y[i]
end

x = ones(SIZE)
y = ones(SIZE)
jx = JACC.Array(x)
jy = JACC.Array(y)
for i in [10,100,1000,10000,100000,1000000,10000000,100000000]
 @time begin
  res = JACC.parallel_reduce(i, dot, jx, jy)
 end
end

#-------------------------2D AXPY

function axpy_oneapi_kernel(alpha::Float64, x, y)
  i = get_global_id(0)
  j = get_global_id(1)
  @inbounds x[i,j] = x[i,j] + alpha * y[i,j]
  return nothing
end

function axpy_oneapi((M,N),alpha::Float64,x,y)
  maxPossibleItems = 16
  Mitems = min(M, maxPossibleItems)
  Nitems = min(N, maxPossibleItems)
  Mgroups = ceil(Int, M / Mitems)
  Ngroups = ceil(Int, N / Nitems)
  oneAPI.@sync @oneapi items = (Mitems, Nitems) groups = (Mgroups, Ngroups) axpy_oneapi_kernel(alpha,x,y)
end

SIZE = 10000
x = ones(Float64,SIZE,SIZE)
y = ones(Float64,SIZE,SIZE)
alpha::Float64 = 2.5
dx = oneArray(x)
dy = oneArray(y)
for i in [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]
 @time begin
  axpy_oneapi((i,i),alpha,dx,dy)
 end
end


function axpy(i, j, alpha, x, y)
    @inbounds x[i,j] = x[i,j] + alpha * y[i,j]
end

x = ones(SIZE, SIZE)
y = ones(SIZE, SIZE)
jx = JACC.Array(x)
jy = JACC.Array(y)
for i in [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]
 @time begin
  JACC.parallel_for((i,i), axpy, alpha, jx, jy)
 end
end

#-------------------------2D DOT

function dot_oneapi_kernel((M, N), ret, x, y)
  shared_mem = oneLocalArray(Float64, 16 * 16)
  i = get_global_id(0)
  j = get_global_id(1)
  ti = get_local_id(0)
  tj = get_local_id(1)
  bi = get_group_id(0)
  bj = get_group_id(1)

  tmp::Float64 = 0.0
  shared_mem[((ti-1)*16)+tj] = tmp

  if (i <= M && j <= N)
    tmp = @inbounds x[i,j] * y[i,j]
    shared_mem[(ti-1)*16+tj] = tmp
  end
  barrier()
  if (ti <= 8 && tj <= 8 && ti+8 <= M && tj+8 <= N)
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti+7)*16)+(tj+8)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti-1)*16)+(tj+8)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti+7)*16)+tj]
  end
  barrier()
  if (ti <= 4 && tj <= 4 && ti+4 <= M && tj+4 <= N)
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti+3)*16)+(tj+4)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti-1)*16)+(tj+4)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti+3)*16)+tj]
  end
  barrier()
  if (ti <= 2 && tj <= 2 && ti+2 <= M && tj+2 <= N)
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti+1)*16)+(tj+2)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti-1)*16)+(tj+2)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti+1)*16)+tj]
  end
  barrier()
  if (ti == 1 && tj == 1 && ti+1 <= M && tj+1 <= N)
    shared_mem[((ti-1)*16)+tj] += shared_mem[ti*16+(tj+1)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[((ti-1)*16)+(tj+1)]
    shared_mem[((ti-1)*16)+tj] += shared_mem[ti*16+tj]
    ret[bi,bj] = shared_mem[((ti-1)*16)+tj]
  end
  return nothing
end

function reduce_kernel((M, N), red, ret)
  shared_mem = oneLocalArray(Float64, 16 * 16)
  i = get_local_id(0)
  j = get_local_id(1)
  ii = i
  jj = j

  tmp::Float64 = 0.0
  shared_mem[(i-1)*16+j] = tmp
  
  if M > 16 && N > 16
    while ii <= M
      jj = get_local_id(1)
      while jj <= N
        tmp = tmp + @inbounds red[ii,jj]
        jj += 16
      end
      ii += 16
    end
  elseif M > 16
    while ii <= N
      tmp = tmp + @inbounds red[ii,jj]
      ii += 16
    end
  elseif N > 16
    while jj <= N
      tmp = tmp + @inbounds red[ii,jj]
      jj += 16
    end
  elseif M <= 16 && N <= 16
    if i <= M && j <= N
      tmp = tmp + @inbounds red[i,j]
    end
  end
  shared_mem[(i-1)*16+j] = tmp
  barrier()
  if (i <= 8 && j <= 8)
    if (i+8 <= M && j+8 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i+7)*16)+(j+8)]
    end
    if (i <= M && j+8 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i-1)*16)+(j+8)]
    end
    if (i+8 <= M && j <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i+7)*16)+j]
    end
  end
  barrier()
  if (i <= 4 && j <= 4)
    if (i+4 <= M && j+4 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i+3)*16)+(j+4)]
    end
    if (i <= M && j+4 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i-1)*16)+(j+4)]
    end
    if (i+4 <= M && j <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i+3)*16)+j]
    end
  end
  barrier()
  if (i <= 2 && j <= 2)
    if (i+2 <= M && j+2 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i+1)*16)+(j+2)]
    end
    if (i <= M && j+2 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i-1)*16)+(j+2)]
    end
    if (i+2 <= M && j <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i+1)*16)+j]
    end
  end
  barrier()
  if (i == 1 && j == 1)
    if (i+1 <= M && j+1 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[i*16+(j+1)]
    end
    if (i <= M && j+1 <= N)
      shared_mem[((i-1)*16)+j] += shared_mem[((i-1)*16)+(j+1)]
    end
    if (i+1 <= M && j <= N)  
      shared_mem[((i-1)*16)+j] += shared_mem[i*16+j]
    end
    ret[1] = shared_mem[((i-1)*16)+j] 
  end
  return nothing
end

function dot_oneapi((M,N), x, y)
  maxPossibleItems = 16 
  Mitems = min(M, maxPossibleItems)
  Nitems = min(N, maxPossibleItems)
  Mgroups = ceil(Int, M/Mitems)
  Ngroups = ceil(Int, N/Nitems)
  ret = oneAPI.zeros(Float64,(Mgroups, Ngroups))
  rret = oneAPI.zeros(Float64,1)
  oneAPI.@sync @oneapi items = (Mitems, Nitems) groups = (Mgroups, Ngroups) dot_oneapi_kernel((M, N), ret, x, y)
  oneAPI.@sync @oneapi items = (Mitems, Nitems) groups = (1, 1) reduce_kernel((Mgroups, Ngroups), ret, rret)
  return rret
end

SIZE = 10000
x = ones(Float64,SIZE,SIZE)
y = ones(Float64,SIZE,SIZE)
dx = oneArray(x)
dy = oneArray(y)
for i in [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]
 @time begin
  res = dot_oneapi((i,i),dx,dy)
 end
end

function dot(i, j, x, y)
  return @inbounds x[i,j] * y[i,j]
end

x = ones(SIZE,SIZE)
y = ones(SIZE,SIZE)
jx = JACC.Array(x)
jy = JACC.Array(y)
for i in [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]
 @time begin
  res = JACC.parallel_reduce((i,i), dot, jx, jy)
 end
end

#-------------------------LBM

function lbm_oneapi_kernel(f, f1, f2, t, w, cx, cy, SIZE)

  x = get_global_id(0)
  y = get_global_id(1)

  u::Float64 = 0.0
  v::Float64 = 0.0
  p::Float64 = 0.0
  feq::Float64 = 0.0
  x_stream::Int32 = 0 
  y_stream::Int32 = 0
  ind::Int32 = 0
  iind::Int32 = 0
  k::Int32 = 0
  one::Float64 = 1.0
  
  if x > 1 && x < SIZE && y > 1 && y < SIZE
    for k in 1:9
      x_stream = x - cx[k]
      y_stream = y - cy[k]
      ind =  ( k - 1 ) * SIZE * SIZE + x * SIZE + y
      iind = ( k - 1 ) * SIZE * SIZE + x_stream * SIZE + y_stream
      f[trunc(Int,ind)] = f1[trunc(Int,iind)] 
    end
    for k in 1:9
      ind =  ( k - 1 ) * SIZE * SIZE + x * SIZE + y
      p = p[1,1] + f[ind]
      u = u[1,1] + f[ind] * cx[k]
      v = v[1,1] + f[ind] * cy[k]
    end
    u = u / p
    v = v / p
    for k in 1:9
      feq = w[k] * p * ( 1.0 + 3.0 * ( cx[k] * u + cy[k] * v ) + ( cx[k] * u + cy[k] * v ) * ( cx[k] * u + cy[k] * v ) - 1.5 * ( ( u * u ) + ( v * v ) ) )
      ind =  ( k - 1 ) * SIZE * SIZE + x * SIZE + y
      iind = ( k - 1 ) * SIZE * SIZE + x_stream * SIZE + y_stream
      f2[trunc(Int32,iind)] = f[trunc(Int32,ind)] * (one - one / t) + feq * one / t
    end
  end
  return nothing
end


function lbm_oneapi((M,N), f, f1, f2, t, w, cx, cy, SIZE)
  maxPossibleThreads = 16 
  Mitems = min(M, maxPossibleThreads)
  Nitems = min(N, maxPossibleThreads)
  Mgroups = ceil(Int, M/Mitems)
  Ngroups = ceil(Int, N/Nitems)
  oneAPI.@sync @oneapi items = (Mitems, Nitems) groups = (Mgroups, Ngroups) lbm_oneapi_kernel(f, f1, f2, t, w, cx, cy, SIZE)
end

SIZE::Int32 = 1000
two::Float64 = 2.0
three::Float64 = 3.0
four::Float64 = 4.0
f = ones(Float64, SIZE * SIZE * 9) .* two
f1 = ones(Float64, SIZE * SIZE * 9) .* three 
f2 = ones(Float64, SIZE * SIZE * 9) .* four
cx = zeros(Int32, 9)
cy = zeros(Int32, 9)
cx[1] = 0
cy[1] = 0
cx[2] = 1
cy[2] = 0
cx[3] = -1
cy[3] = 0
cx[4] = 0
cy[4] = 1
cx[5] = 0
cy[5] = -1
cx[6] = 1
cy[6] = 1
cx[7] = -1
cy[7] = 1
cx[8] = -1
cy[8] = -1
cx[9] = 1
cy[9] = -1
 
w   = ones(Float64,9)
t::Float64   = 1.0
df  = oneArray(f)
df1 = oneArray(f1)
df2 = oneArray(f2)
dcx = oneArray(cx)
dcy = oneArray(cy)
dw  = oneArray(w)

for i in [100,200,300,400,500,600,700,800,900,1000]
 @time begin
  lbm_oneapi((i,i),df,df1,df2,t,dw,dcx,dcy,i)
 end
end

function lbm(x, y, f, f1, f2, t, w, cx, cy, SIZE)
  u = 0.0
  v = 0.0
  p = 0.0
  x_stream = 0 
  y_stream = 0
  
  if x > 1 && x < SIZE && y > 1 && y < SIZE
    for k in 1:9
      x_stream = x - cx[k]
      y_stream = y - cy[k]
      ind =  ( k - 1 ) * SIZE * SIZE + x * SIZE + y
      iind = ( k - 1 ) * SIZE * SIZE + x_stream * SIZE + y_stream
      f[trunc(Int,ind)] = f1[trunc(Int,iind)]
    end
    for k in 1:9
      ind =  ( k - 1 ) * SIZE * SIZE + x * SIZE + y
      p = p[1,1] + f[ind]
      u = u[1,1] + f[ind] * cx[k]
      v = v[1,1] + f[ind] * cy[k]
    end
    u = u / p
    v = v / p
    for k in 1:9
      feq = w[k] * p * ( 1.0 + 3.0 * ( cx[k] * u + cy[k] * v ) + ( cx[k] * u + cy[k] * v ) * ( cx[k] * u + cy[k] * v ) - 1.5 * ( ( u * u ) + ( v * v ) ) )
      ind =  ( k - 1 ) * SIZE * SIZE + x * SIZE + y
      iind = ( k - 1 ) * SIZE * SIZE + x_stream * SIZE + y_stream
      f2[trunc(Int,iind)] = f[trunc(Int,ind)] * (1.0 - 1.0 / t) + feq * 1 / t
    end
  end
end

f = ones(SIZE * SIZE * 9) .* 2.0
f1 = ones(SIZE * SIZE * 9) .* 3.0 
f2 = ones(SIZE * SIZE * 9) .* 4.0
w   = ones(9)
t   = 1.0
jf  = JACC.Array(f)
jf1 = JACC.Array(f1)
jf2 = JACC.Array(f2)
jcx = JACC.Array(cx)
jcy = JACC.Array(cy)
jw  = JACC.Array(w)

for i in [100,200,300,400,500,600,700,800,900,1000]
 @time begin
   JACC.parallel_for((i,i),lbm,jf,jf1,jf2,t,jw,jcx,jcy,SIZE)
 end
end

#-------------------------CG

function axpy_oneapi_kernel(alpha::Float32,x,y)
  i = get_global_id()
  @inbounds x[i] = x[i] + alpha * y[i]
  return nothing
end

function axpy_oneapi(SIZE,alpha::Float32,x,y)
  maxPossibleItems = 256
  items = min(SIZE, maxPossibleItems)
  groups = ceil(Int, SIZE / items)
  oneAPI.@sync @oneapi items = items groups = groups axpy_oneapi_kernel(alpha,x,y)
end

function dot_oneapi_kernel(SIZE, ret, x, y)
  shared_mem = oneLocalArray(Float64, 256)
  i = get_global_id(0)
  ti = get_local_id(0)
  tmp::Float64 = 0.0
  shared_mem[ti] = 0.0
  if i <= SIZE
    tmp = @inbounds x[i] * y[i]
    shared_mem[ti] = tmp
  end
  barrier() 
  if (ti <= 128)
    shared_mem[ti] += shared_mem[ti+128]
  end
  barrier() 
  if (ti <= 64)
    shared_mem[ti] += shared_mem[ti+64]
  end
  barrier()
  if (ti <= 32)
    shared_mem[ti] += shared_mem[ti+32]
  end
  barrier()
  if (ti <= 16)
    shared_mem[ti] += shared_mem[ti+16]
  end
  barrier()
  if (ti <= 8)
    shared_mem[ti] += shared_mem[ti+8]
  end
  barrier()
  if (ti <= 4)
    shared_mem[ti] += shared_mem[ti+4]
  end
  barrier()
  if (ti <= 2)
    shared_mem[ti] += shared_mem[ti+2]
  end
  barrier()
  if (ti == 1)
    shared_mem[ti] += shared_mem[ti+1]
    ret[get_group_id(0)] = shared_mem[ti]
  end
  barrier()
  return nothing
end

function reduce_kernel(SIZE, red, ret)
  shared_mem = oneLocalArray(Float64, 256)
  i = get_global_id(0)
  ii = i
  tmp::Float64 = 0.0
  if SIZE > 256
    while ii <= SIZE
      tmp += @inbounds red[ii]
      ii += 256
    end
  else
    tmp = @inbounds red[i]
  end
  shared_mem[i] = tmp
  barrier()
  if (i <= 128)
    shared_mem[i] += shared_mem[i+128]
  end
  barrier()
  if (i <= 64)
    shared_mem[i] += shared_mem[i+64]
  end
  barrier()
  if (i <= 32)
    shared_mem[i] += shared_mem[i+32]
  end
  barrier()
  if (i <= 16)
    shared_mem[i] += shared_mem[i+16]
  end
  barrier()
  if (i <= 8)
    shared_mem[i] += shared_mem[i+8]
  end
  barrier()
  if (i <= 4)
    shared_mem[i] += shared_mem[i+4]
  end
  barrier()
  if (i <= 2)
    shared_mem[i] += shared_mem[i+2]
  end
  barrier()
  if (i == 1)
    shared_mem[i] += shared_mem[i+1]
    ret[1] = shared_mem[1]
  end
  return nothing
end

function dot_oneapi(SIZE, x, y)
  numItems = 256
  items = min(SIZE, numItems)
  groups = ceil(Int, SIZE/items)
  ret = oneAPI.zeros(Float32, groups)
  rret = oneAPI.zeros(Float32, 1)
  oneAPI.@sync @oneapi items = items groups = groups dot_oneapi_kernel(SIZE, ret, x, y)
  oneAPI.@sync @oneapi items = items groups = 1 reduce_kernel(SIZE, ret, rret)
  return rret
end

function matvecmul_oneapi_kernel(SIZE, a3, a2, a1, x, y)
  i = get_global_id()
    if i == 1
      @inbounds y[i] = a2[i] * x[i] + a1[i] * x[i+1]
    elseif i == length(x)
      @inbounds y[i] = a3[i] * x[i-1] + a2[i] * x[i]
    else
      @inbounds y[i] = a3[i] * x[i-1] + a2[i] * x[i] + a1[i] * x[i+1]
    end
  return nothing
end

function matvecmul_oneapi(SIZE, a3, a2, a1, x, y)
  maxPossibleItems = 256
  items = min(SIZE, maxPossibleItems)
  groups = ceil(Int, SIZE / items)
  oneAPI.@sync @oneapi items = items groups = groups matvecmul_oneapi_kernel(SIZE, a3, a2, a1, x, y)
end

function cg_oneapi(SIZE, a3, a2, a1, r, p, s, x, r_old, r_aux )

  zero_four::Float64 = 0.4
  zero_five::Float64 = 0.5
  a1 = a1 * zero_four
  r = r * zero_five
  p = p * zero_five
  alpha::Float64 = 0.0
  negative_alpha::Float64 = 0.0
  beta::Float64 = 0.0
  minus_one::Float64 = -1.0

  for i in 1:1

    r_old = copy(r) 

    matvecmul_oneapi(SIZE, a3, a2, a1, p, s)

    alpha0 = dot_oneapi(SIZE, r, r)
    alpha1 = dot_oneapi(SIZE, p, s)

    aalpha0::Float64 = alpha0[1::Integer]
    aalpha1::Float64 = alpha1[1::Integer]

    alpha = aalpha0 / aalpha1
    negative_alpha = alpha * minus_one

    axpy_oneapi(SIZE, negative_alpha, r, s)
    axpy_oneapi(SIZE, alpha, x, p)

    beta0 = dot_oneapi(SIZE, r, r)
    beta1 = dot_oneapi(SIZE, r_old, r_old)
    
    bbeta0::Float64 = beta0[1::Integer]
    bbeta1::Float64 = beta1[1::Integer]
    
    beta = bbeta0 / bbeta1

    r_aux = copy(r)

    axpy_oneapi(SIZE, beta, r_aux,p)
    cond = dot_oneapi(SIZE, r, r)

    p = copy(r_aux)

  end
end

SIZE = 100000000
a3 = ones(Float64, SIZE)
a2 = ones(Float64, SIZE)
a1 = ones(Float64, SIZE)
r = ones(Float64, SIZE)
p = ones(Float64, SIZE)
s = zeros(Float64, SIZE)
x = zeros(Float64, SIZE)
r_old = zeros(Float64, SIZE)
r_aux = zeros(Float64, SIZE)
da3 = oneArray(a3)
da2 = oneArray(a2)
da1 = oneArray(a1)
dr = oneArray(r)
dp = oneArray(p)
ds = oneArray(s)
dx = oneArray(x)
dr_old = oneArray(r_old)
dr_aux = oneArray(r_aux)

@time begin
  cg_oneapi(SIZE, da3, da2, da1, dr, dp, ds, dx, dr_old, dr_aux)
end

function matvecmul(i, a3, a2, a1, x, y)
  if i == 1
    @inbounds y[i] = a2[i] * x[i] + a1[i] * x[i+1]
  elseif i == length(x)
    @inbounds y[i] = a3[i] * x[i-1] + a2[i] * x[i]
  else
    @inbounds y[i] = a3[i] * x[i-1] + a2[i] * x[i] + a1[i] * x[i+1]
  end
end

function axpy(i, alpha, x, y)
  @inbounds x[i] += alpha * y[i]
end

function dot(i, x, y)
  return  @inbounds x[i] * y[i]
end

function cg(SIZE, a3, a2, a1, r, p, s, x, r_old, r_aux )

  a1 = a1 * 4.0
  r = r * 0.5
  p = p * 0.5
  alpha::Float64 = 0.0
  negative_alpha::Float64 = 0.0
  beta::Float64 = 0.0

  for i in 1:1

    r_old = copy(r) 

    JACC.parallel_for(SIZE, matvecmul, a3, a2, a1, p, s)

    alpha0 = JACC.parallel_reduce(SIZE, dot, r, r)
    alpha1 = JACC.parallel_reduce(SIZE, dot, p, s)
    
    aalpha0::Float64 = alpha0[1::Integer]
    aalpha1::Float64 = alpha1[1::Integer]

    alpha = aalpha0 / aalpha1
    negative_alpha = alpha * (-1.0)

    JACC.parallel_for(SIZE, axpy, negative_alpha, r, s)
    JACC.parallel_for(SIZE, axpy, alpha, x, p)

    beta0 = JACC.parallel_reduce(SIZE, dot, r, r)
    beta1 = JACC.parallel_reduce(SIZE, dot, r_old, r_old)
    bbeta0::Float64 = beta0[1::Integer]
    bbeta1::Float64 = beta1[1::Integer]
    
    beta = bbeta0 / bbeta1

    r_aux = copy(r)

    JACC.parallel_for(SIZE, axpy, beta, r_aux, p)
    cond= JACC.parallel_reduce(SIZE, dot, r, r)

    p = copy(r_aux)

  end
end

a3 = ones(SIZE)
a2 = ones(SIZE)
a1 = ones(SIZE)
r = ones(SIZE)
p = ones(SIZE)
s = zeros(SIZE)
x = zeros(SIZE)
r_old = zeros(SIZE)
r_aux = zeros(SIZE)
ja3 = JACC.Array(a3)
ja2 = JACC.Array(a2)
ja1 = JACC.Array(a1)
jr = JACC.Array(r)
jp = JACC.Array(p)
js = JACC.Array(s)
jx = JACC.Array(x)
jr_old = JACC.Array(r_old)
jr_aux = JACC.Array(r_aux)


@time begin
 cg(SIZE, ja3, ja2, ja1, jr, jp, js, jx, jr_old, jr_aux)
end
